# Penetron

[English](#english) | [ä¸­æ–‡](#chinese)

<a name="english"></a>

## ğŸ‡¬ğŸ‡§ English Description

**Penetron** is a framework designed to train and evaluate Large Language Models (LLMs) for the task of translating natural language instructions into effective Linux/Penetration Testing terminal commands.

The project implements a two-stage training pipeline (SFT + GRPO) powered by [Unsloth](https://github.com/unslothai/unsloth) for efficiency and [SwanLab](https://swanlab.cn/) for experiment tracking. It also includes a robust evaluation script supporting multiple models and datasets.

### âœ¨ Key Features

*   **Stage 1: Supervised Fine-Tuning (SFT):** Trains the model to understand the task and generate Chain-of-Thought (CoT) reasoning using `<think>` tags.
*   **Stage 2: Group Relative Policy Optimization (GRPO):** Aligns the model using Reinforcement Learning. Custom reward functions penalize incorrect formats and reward accurate command generation (Syntax & Execution correctness simulation).
*   **Efficient Training:** Utilizes **Unsloth** for faster training and lower memory usage (LoRA/QLoRA).
*   **Concurrent Evaluation:** A multi-threaded evaluator compatible with OpenAI-API format (e.g., vLLM, SGLang) to benchmark various models against standard datasets (NL2SH, Implicit/Explicit Tools).

### ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ config.json           # Model configurations for evaluation
â”œâ”€â”€ evaluator.py          # Main evaluation script (Concurrent & OpenAI-API compatible)
â”œâ”€â”€ sft_stage1.py         # Stage 1: SFT Training script
â”œâ”€â”€ grpo_stage2.py        # Stage 2: GRPO/RL Training script
â”œâ”€â”€ .env                  # Environment variables for evaluation
â”œâ”€â”€ training_data/        # Directory for training data
â”‚   â”œâ”€â”€ sft_think.jsonl   # Data for Stage 1
â”‚   â””â”€â”€ grpo_train.jsonl  # Data for Stage 2
â””â”€â”€ model/                # Directory for saved models
```

### ğŸš€ Getting Started

#### 1. Prerequisites

*   Python 3.10+
*   PyTorch (CUDA supported)
*   Unsloth
*   TRL, Transformers, Datasets
*   SwanLab (for logging)

```bash
pip install unsloth "unsloth[colab-new]" @ git+https://github.com/unslothai/unsloth.git
pip install --no-deps trl peft accelerate bitsandbytes
pip install swanlab pandas openai rapidfuzz python-dotenv
```

#### 2. Data Preparation

Prepare your datasets in JSONL format and place them in the `training_data` folder.

*   **SFT Data (`sft_think.jsonl`):**
    ```json
    {"input": "Task description...", "reasoning": "Reasoning steps...", "answer": "ls -la"}
    ```
*   **GRPO Data (`grpo_train.jsonl`):**
    ```json
    {"input": "Task description...", "answer": "ls -la"}
    ```

#### 3. Training

**Stage 1: Supervised Fine-Tuning**

```bash
python sft_stage1.py
```
*   This will finetune a base model (default: `Qwen/Qwen2.5-7B-Instruct`) and save the adapter to `./model/sft/final_model`.

**Stage 2: GRPO Alignment**

> **Note:** Ensure `grpo_stage2.py` points to the correct path of the model trained in Stage 1.

```bash
python grpo_stage2.py
```
*   This loads the SFT model and optimizes it using reward functions (Format Reward + Accuracy Reward).

#### 4. Evaluation

The evaluator works by sending requests to an LLM inference server (like vLLM) that mimics the OpenAI API.

**Configuration:**

1.  Edit `.env` to set your target dataset and API keys.
    ```ini
    DATASET=all_qa  # or 'westen', 'Implicit', 'Explicit'
    OUTPUT_DIR=results
    OPENAI_BASE_URL=http://localhost:8000/v1
    OPENAI_API_KEY=EMPTY
    MAX_WORKERS=10  # Adjust based on your API throughput
    ```
2.  Edit `config.json` to define the models you want to evaluate.

**Run Evaluation:**

```bash
python evaluator.py
```

The script calculates:
*   **Exact Match**
*   **Keyword Recall**
*   **Jaccard Similarity**
*   **Levenshtein Similarity**

Results are saved as CSV files in the `results/` directory.

---

<a name="chinese"></a>

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡è¯´æ˜

**Penetron** æ—¨åœ¨å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºæœ‰æ•ˆçš„ Linux æˆ–æ¸—é€æµ‹è¯•ç»ˆç«¯å‘½ä»¤ã€‚

è¯¥é¡¹ç›®å®ç°äº†ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼ˆSFT + GRPOï¼‰ï¼Œåˆ©ç”¨ [Unsloth](https://github.com/unslothai/unsloth) è¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼Œå¹¶ä½¿ç”¨ [SwanLab](https://swanlab.cn/) è¿›è¡Œå®éªŒè·Ÿè¸ªã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…å«ä¸€ä¸ªæ”¯æŒå¤šæ¨¡å‹å¯¹æ¯”çš„å¼ºå¤§è¯„ä¼°è„šæœ¬ã€‚

### âœ¨ ä¸»è¦ç‰¹æ€§

*   **ç¬¬ä¸€é˜¶æ®µï¼šç›‘ç£å¾®è°ƒ (SFT):** è®­ç»ƒæ¨¡å‹ç†è§£ä»»åŠ¡å¹¶åˆ©ç”¨ `<think>` æ ‡ç­¾ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹ã€‚
*   **ç¬¬äºŒé˜¶æ®µï¼šç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (GRPO):** ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¯¹æ¨¡å‹è¿›è¡Œå¯¹é½ã€‚å†…ç½®è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œç”¨äºæƒ©ç½šé”™è¯¯æ ¼å¼å¹¶å¥–åŠ±å‡†ç¡®çš„å‘½ä»¤ç”Ÿæˆï¼ˆåŸºäºæ ‡å‡†åŒ–å‘½ä»¤çš„åŒ¹é…åº¦ï¼‰ã€‚
*   **é«˜æ•ˆè®­ç»ƒ:** åˆ©ç”¨ **Unsloth** å®ç°æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ (æ”¯æŒ LoRA/QLoRA)ã€‚
*   **å¹¶å‘è¯„ä¼°:** åŸºäº OpenAI-API æ ¼å¼ï¼ˆå…¼å®¹ vLLM, SGLang ç­‰ï¼‰çš„å¤šçº¿ç¨‹è¯„ä¼°å™¨ï¼Œæ”¯æŒåœ¨æ ‡å‡†æ•°æ®é›†ï¼ˆNL2SH, Implicit/Explicit Toolsï¼‰ä¸Šå¯¹å¤šä¸ªæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

### ğŸ“‚ é¡¹ç›®ç»“æ„

```text
.
â”œâ”€â”€ config.json           # è¯„ä¼°ç”¨çš„æ¨¡å‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ evaluator.py          # ä¸»è¯„ä¼°è„šæœ¬ï¼ˆæ”¯æŒå¹¶å‘ & OpenAI-APIï¼‰
â”œâ”€â”€ sft_stage1.py         # ç¬¬ä¸€é˜¶æ®µï¼šSFT è®­ç»ƒè„šæœ¬
â”œâ”€â”€ grpo_stage2.py        # ç¬¬äºŒé˜¶æ®µï¼šGRPO/RL è®­ç»ƒè„šæœ¬
â”œâ”€â”€ .env                  # è¯„ä¼°ç”¨çš„ç¯å¢ƒå˜é‡
â”œâ”€â”€ training_data/        # è®­ç»ƒæ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ sft_think.jsonl   # SFT é˜¶æ®µæ•°æ®
â”‚   â””â”€â”€ grpo_train.jsonl  # GRPO é˜¶æ®µæ•°æ®
â””â”€â”€ model/                # æ¨¡å‹ä¿å­˜ç›®å½•
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### 1. ç¯å¢ƒä¾èµ–

*   Python 3.10+
*   PyTorch (æ”¯æŒ CUDA)
*   Unsloth
*   TRL, Transformers, Datasets
*   SwanLab (ç”¨äºæ—¥å¿—è®°å½•)

```bash
pip install unsloth "unsloth[colab-new]" @ git+https://github.com/unslothai/unsloth.git
pip install --no-deps trl peft accelerate bitsandbytes
pip install swanlab pandas openai rapidfuzz python-dotenv
```

#### 2. æ•°æ®å‡†å¤‡

è¯·å‡†å¤‡ JSONL æ ¼å¼çš„æ•°æ®é›†å¹¶å°†å…¶æ”¾å…¥ `training_data` æ–‡ä»¶å¤¹ã€‚

*   **SFT æ•°æ® (`sft_think.jsonl`):**
    ```json
    {"input": "ä»»åŠ¡æè¿°...", "reasoning": "æ¨ç†æ­¥éª¤...", "answer": "ls -la"}
    ```
*   **GRPO æ•°æ® (`grpo_train.jsonl`):**
    ```json
    {"input": "ä»»åŠ¡æè¿°...", "answer": "ls -la"}
    ```

#### 3. è®­ç»ƒæµç¨‹

**ç¬¬ä¸€é˜¶æ®µï¼šç›‘ç£å¾®è°ƒ (SFT)**

```bash
python sft_stage1.py
```
*   è¯¥è„šæœ¬å°†å¾®è°ƒåŸºç¡€æ¨¡å‹ï¼ˆé»˜è®¤ï¼š`Qwen/Qwen2.5-7B-Instruct`ï¼‰å¹¶å°†é€‚é…å™¨ä¿å­˜åˆ° `./model/sft/final_model`ã€‚

**ç¬¬äºŒé˜¶æ®µï¼šGRPO å¯¹é½**

> **æ³¨æ„:** è¯·ç¡®ä¿ `grpo_stage2.py` ä¸­çš„æ¨¡å‹è·¯å¾„æŒ‡å‘ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ã€‚

```bash
python grpo_stage2.py
```
*   åŠ è½½ SFT æ¨¡å‹å¹¶åˆ©ç”¨å¥–åŠ±å‡½æ•°ï¼ˆæ ¼å¼å¥–åŠ± + å‡†ç¡®æ€§å¥–åŠ±ï¼‰è¿›è¡Œä¼˜åŒ–ã€‚

#### 4. æ¨¡å‹è¯„ä¼°

è¯„ä¼°å™¨é€šè¿‡å‘æ¨¡æ‹Ÿ OpenAI API çš„æ¨ç†æœåŠ¡å™¨ï¼ˆå¦‚ vLLMï¼‰å‘é€è¯·æ±‚æ¥å·¥ä½œã€‚

**é…ç½®:**

1.  ç¼–è¾‘ `.env` æ–‡ä»¶è®¾ç½®æ•°æ®é›†å’Œ API å¯†é’¥ã€‚
    ```ini
    DATASET=all_qa  # å¯é€‰ 'westen', 'Implicit', 'Explicit' æˆ– 'all_qa'
    OUTPUT_DIR=results
    OPENAI_BASE_URL=http://localhost:8000/v1
    OPENAI_API_KEY=EMPTY
    MAX_WORKERS=10  # æ ¹æ®ä½ çš„ API ååé‡è°ƒæ•´å¹¶å‘æ•°
    ```
2.  ç¼–è¾‘ `config.json` å®šä¹‰éœ€è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨ã€‚

**è¿è¡Œè¯„ä¼°:**

```bash
python evaluator.py
```

è„šæœ¬å°†è®¡ç®—ä»¥ä¸‹æŒ‡æ ‡ï¼š
*   **Exact Match (å®Œå…¨åŒ¹é…)**
*   **Keyword Recall (å…³é”®è¯å¬å›ç‡)**
*   **Jaccard Similarity (Jaccard ç›¸ä¼¼åº¦)**
*   **Levenshtein Similarity (ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦)**

è¯¦ç»†ç»“æœå’Œæ±‡æ€»æŠ¥å‘Šå°†ä»¥ CSV æ ¼å¼ä¿å­˜åœ¨ `results/` ç›®å½•ä¸‹ã€‚
